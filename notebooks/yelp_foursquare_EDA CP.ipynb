{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "import geopy\n",
    "from geopy import distance\n",
    "from geopy.distance import geodesic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foursquare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send a request to Foursquare with a small radius (1000m) for all the bike stations in your city of choice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FourSquare doesn't have pagination OR offset, which complicates doing calls to compare to Yelp. I was able to get ALL the results\n",
    "    from Yelp, so if I set the radius too large here, I won't get all the results in the radius because they limit their pulls to 50.\n",
    "\n",
    "I think 1000m is too large in Helsinki, Finland: my city of choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api call for each station\n",
    "for i in range(0,122):\n",
    "    # prepare the lat and long\n",
    "    x = bike_yelp_small[\"latitude\"][i]\n",
    "    lat = str(x)\n",
    "    y = bike_yelp_small[\"longitude\"][i]\n",
    "    long = str(y)\n",
    "\n",
    "    url = \"https://api.foursquare.com/v3/places/search\"\n",
    "\n",
    "    params = {\n",
    "        \"ll\": f\"{lat},{long}\",\n",
    "        \"radius\":\"500\",\n",
    "        \"categories\":\"13065,16000,10000\",\n",
    "        \"sort\":\"DISTANCE\",\n",
    "        \"limit\":\"50\"\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"Authorization\": <API_KEY>\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"GET\", url, params=params, headers=headers)\n",
    "\n",
    "    data = response.text\n",
    "    parse_json = json.loads(data)\n",
    "    molecule = (parse_json['results'])\n",
    "    \n",
    "    # save response\n",
    "    four_temp = pd.DataFrame(molecule)\n",
    "    # Appending the temporary DataFrame to the main DataFrame\n",
    "    four_df = pd.concat([four_df, four_temp], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This took a LOT of trouble shooting. I can't explain the trouble shooting that occurred. I have never written anything like this ever,\n",
    "so when I finally got this to run I was absolutely ECSTATIC. The above is the finished franken-code that finally worked. I'm positive it \n",
    "is inefficient, and UGLY. BUT it WORKS!! 'Molecule' as a name above is a relic from when I was naming files based on assuming it wouldn't \n",
    "work again, and I left it there to remind me that it will work eventually if you keep changing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for duplicates in the finished data. The original database four_df has 6032 rows, and fallback_four (duplicates removed), has 6032. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fallback_four = four_df[['categories','distance','geocodes','name']]\n",
    "fallback_four.drop_duplicates('name')\n",
    "fallback_four"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save this data into csv, it was hard won!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "four_df.to_csv(r'C:\\Users\\ca0pa\\Desktop\\LHL\\Python\\four_df_5500rows.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we need to dig a bit further into the data and pull out the lat/long data, then rejoin the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "four_rest_coord = pd.DataFrame(pd.json_normalize(fallback_four['geocodes']))\n",
    "four_rest_coo = four_rest_coord[['main.latitude','main.longitude']]\n",
    "four_rest_coo.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop any extra columns that you do not need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "four_wlatlong = pd.concat([fallback_four, four_rest_coo], axis=1, join=\"inner\")\n",
    "four_wlatlong.drop(columns='categories',inplace=True)\n",
    "four_wlatlong.drop(columns='geocodes',inplace=True)\n",
    "four_wlatlong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Finally, create that mergevar that will allow you to combine this data in all the crazy ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "four_wlatlong['mergevar'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code troubleshooting below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = bike_yelp_small[\"latitude\"][1]\n",
    "lat = str(x)\n",
    "lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_yelp_small[\"latitude\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = bike_yelp_small[\"longitude\"][i]\n",
    "long = str(y)\n",
    "long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://api.foursquare.com/v3/places/search?ll%3D\"+lat+\"%2C\"+long+\"&radius=500&categories=13065%2C16000%2C10000&sort=DISTANCE&limit=50\"\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = bike_yelp_small[\"latitude\"][15]\n",
    "lat = str(x)\n",
    "y = bike_yelp_small[\"longitude\"][15]\n",
    "long = str(y)\n",
    "url = \"https://api.foursquare.com/v3/places/search?ll%3D\"+lat+\"%2C\"+long+\"&radius=500&categories=13065%2C16000%2C10000&sort=DISTANCE&limit=50\"\n",
    "headers = {\n",
    "    \"Accept\": \"application/json\",\n",
    "    \"Authorization\": <API_KEY>\n",
    "}\n",
    "data = response.text\n",
    "parse_json = json.loads(data)\n",
    "molecule = (parse_json['results'])\n",
    "    \n",
    "    # save response\n",
    "four_temp = pd.DataFrame(molecule)\n",
    "    # Appending the temporary DataFrame to the main DataFrame\n",
    "four_df = pd.concat([four_df, four_temp], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range(len(bike_yelp_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "four_four = pd.concat([four_temp, four_df], ignore_index=True)\n",
    "four_four.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so, 'data' is txt 'molecule' is list and 'parse_json' is dict\n",
    "# pd.read_json DOES NOT WORK\n",
    "data\n",
    "parse_json = json.loads(data)\n",
    "type(parse_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0,10):\n",
    "    # prepare the lat and long\n",
    "    x = bike_yelp_small[\"latitude\"][15]\n",
    "    lat = str(x)\n",
    "    y = bike_yelp_small[\"longitude\"][15]\n",
    "    long = str(y)\n",
    "    station = bike_yelp_small[\"station\"][15]\n",
    "    url = \"https://api.foursquare.com/v3/places/search?ll%3D\"+lat+\"%2C\"+long+\"&radius=500&categories=13065%2C16000%2C10000&sort=DISTANCE&limit=50\"\n",
    "    headers = {\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"Authorization\": <API_KEY>\n",
    "    }\n",
    "    print(lat,long,station,url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load json object\n",
    "\n",
    "with open('FS_center.json') as f:\n",
    "    FS_center = json.load(f)\n",
    "FS_center = pd.DataFrame(FS_center['results'])\n",
    "FS_center.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foursquare = pd.DataFrame(data['results'])[\n",
    "        ['fsq_id', 'categories', 'distance', 'geocodes']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index for your source lat/long file, the code can't run through the index if there are numbers missing.\n",
    "bike_yelp_small = bike_yelp_small.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send a request to Yelp with a small radius (1000m) for all the bike stations in your city of choice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First 'get' for restaurants in Helsinki. I know we were not supposed to do it this way, but I worked from the center of Helsinki and got \n",
    "all the results for all restaurants within 3.75kms of that center point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = \"https://api.yelp.com/v3/businesses/search?location=Helsinki&term=restaurants&sort_by=distance&limit=50\"\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"Authorization\": \"bearer <API_KEY>\"\n",
    "    }\n",
    "response = requests.get(url, headers=headers)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate all the results from all of the calls within the diameter of the center of Helsinki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelpconcat = (yelp_restaurant,yelp_restaurant2,yelp_restaurant3,yelp_restaurant4,yelp_restaurant5,yelp_restaurant6,yelp_restaurant7,\n",
    "             yelp_restaurant8,yelp_restaurant9,yelp_restaurant10,yelp_restaurant11,yelp_restaurant12,yelp_restaurant13,yelp_restaurant14,\n",
    "             yelp_restaurant15,yelp_restaurant16,yelp_restaurant17,yelp_restaurant18,yelp_restaurant19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, drop all the columns you don't need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_restaurants = pd.concat(yelpconcat,ignore_index=True)\n",
    "yelp_restaurants.drop(columns='alias',inplace=True)\n",
    "yelp_restaurants.drop(columns='image_url',inplace=True)\n",
    "yelp_restaurants.drop(columns='url',inplace=True)\n",
    "yelp_restaurants.drop(columns='transactions',inplace=True)\n",
    "yelp_restaurants.drop(columns='phone',inplace=True)\n",
    "yelp_restaurants.drop(columns='display_phone',inplace=True)\n",
    "yelp_restaurants.drop(columns='is_closed',inplace=True)\n",
    "yelp_restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#And save it!\n",
    "yelp_restaurants.to_csv(r'C:\\Users\\ca0pa\\Desktop\\LHL\\Python\\yelp_restaurants.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse through the response to get the POI (such as restaurants, bars, etc) details you want (ratings, name, location, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load json object\n",
    "with open('yelp01.json') as f:\n",
    "    yelp_restaurant = json.load(f)\n",
    "yelp_restaurant = pd.DataFrame(yelp_restaurant['businesses'])\n",
    "yelp_rest = pd.json_normalize(yelp_restaurant['coordinates'])\n",
    "yelp_restaurant.head()\n",
    "# bars_2 = pd.DataFrame(bars_1['RangeIndex'])\n",
    "# yelp_restaurant.explode('coordinates')\n",
    "# bikes = pd.DataFrame(bikes['network']['stations'])\n",
    "# bikes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I tried to normalize the column 'coordinates' and keep it in the dataframe, but for some reason I can't explain, the\n",
    "#### panda doesn't want to do that. So, I'll have to normalize the variable and then add it back to the dataframe. ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_rest_cood = pd.DataFrame(pd.json_normalize(yelp_restaurants['coordinates']))\n",
    "yelp_rest_cood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.concat([yelp_restaurants, yelp_rest_cood], axis=1, join=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_restaurants = pd.DataFrame(test)\n",
    "yelp_restaurants.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now get rid of what we can't use in the join. 'id', 'coordinates', 'location', 'price'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_restaurants.drop(columns='id',inplace=True)\n",
    "yelp_restaurants.drop(columns='coordinates',inplace=True)\n",
    "yelp_restaurants.drop(columns='location',inplace=True)\n",
    "yelp_restaurants.drop(columns='price',inplace=True)\n",
    "yelp_restaurants.drop(columns='categories',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, check for duplicate rows that might have been called twice from the api There are None!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_restaurants.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To do what I want to do, I'll have the change the names of the latitude and longitude columns in this data.\n",
    "yelp_restaurants = pd.DataFrame(yelp_restaurants.rename(columns={\"latitude\": \"lat\", \"longitude\": \"lon\", \"name\":\"rest_name\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#and, finally, create the variable for the merge\n",
    "yelp_restaurants['mergevar'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put your parsed results into a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yelp_restaurants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which API provided you with more complete data? Provide an explanation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though Yelp was THE WORST to work with as far as their user interface on their website, and the fact that they made you make a \n",
    "consumer Yelp account, which is annoying, their API extraction was superior in that their search function readily took keywords instead\n",
    "of category numbers, and they had an offset variable so you could get more than 50 results from a search point.\n",
    "\n",
    "In this way, even though there were far fewer results, it may make more sense in some cases to use this data source over FourSquare.\n",
    "\n",
    "However, the FourSquare data was far more extensive. The search radius provided more than 6x as many results as Yelp."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
